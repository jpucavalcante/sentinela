{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XBvAxPoabQ09"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from spacy.lang.pt import Portuguese\n",
    "from spacy.lang.pt import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meu nivel de amizade com isis é ela ter meu in...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>o cara adultera dados que foram desmascarados...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>o cara só é simplesmente o maior vencedor d...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eu to chorando vei vsf e eu nem staneio izone ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eleitor do bolsonaro é tão ignorante q não per...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vai responder as outras 75 conversas e para de...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tem um do jack com a msm música e agr não sei ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mais é ruim pra pedir desafio esse técnico do ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>eu fico vendo isso e penso desvantagens do kp...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>frio do caralho parece até q to dentro do teu ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  meu nivel de amizade com isis é ela ter meu in...    1.0\n",
       "1   o cara adultera dados que foram desmascarados...    1.0\n",
       "2     o cara só é simplesmente o maior vencedor d...    1.0\n",
       "3  eu to chorando vei vsf e eu nem staneio izone ...    1.0\n",
       "4  eleitor do bolsonaro é tão ignorante q não per...    1.0\n",
       "5  vai responder as outras 75 conversas e para de...    1.0\n",
       "6  tem um do jack com a msm música e agr não sei ...    0.0\n",
       "7  mais é ruim pra pedir desafio esse técnico do ...    1.0\n",
       "8   eu fico vendo isso e penso desvantagens do kp...    1.0\n",
       "9  frio do caralho parece até q to dentro do teu ...    1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"comentarios_tratados.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nLw-oNDut84t"
   },
   "outputs": [],
   "source": [
    "# df[\"toxic\"] = df[\"homophobia\"] + df[\"obscene\"] + df[\"insult\"] + df[\"racism\"] + df[\"misogyny\"] + df[\"xenophobia\"]\n",
    "# df.drop(['obscene','insult','racism','misogyny','xenophobia','homophobia'],axis=1,inplace=True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YfbAYE_Tt87t"
   },
   "outputs": [],
   "source": [
    "# df.loc[df['toxic'] == 0, 'toxic'] = 0\n",
    "# df.loc[df['toxic'] > 0, 'toxic'] = 1\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meu nivel de amizade com isis e ela ter meu in...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>o cara adultera dados que foram desmascarados...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>o cara so e simplesmente o maior vencedor d...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eu to chorando vei vsf e eu nem staneio izone ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eleitor do bolsonaro e tao ignorante q nao per...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  meu nivel de amizade com isis e ela ter meu in...    1.0\n",
       "1   o cara adultera dados que foram desmascarados...    1.0\n",
       "2     o cara so e simplesmente o maior vencedor d...    1.0\n",
       "3  eu to chorando vei vsf e eu nem staneio izone ...    1.0\n",
       "4  eleitor do bolsonaro e tao ignorante q nao per...    1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "df['text'] = df['text'].apply(lambda x: unicodedata.normalize('NFKD', x).encode('ASCII','ignore').decode('ASCII'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_special_characters(tweet):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)       # remove os links do tweet\n",
    "    tweet = re.sub(r't.co\\S+', '', tweet)       # remove os links \n",
    "    tweet = re.sub(r'#\\S+', '', tweet)          # remove hashtags\n",
    "    tweet = re.sub(r\"[.]\\s+\", '', tweet)        # remove reticências\n",
    "    emojis = re.compile(\"[\"                     # remove os emojis\n",
    "                      u\"\\U0001F600-\\U0001F64F\"  \n",
    "                      u\"\\U0001F300-\\U0001F5FF\"  \n",
    "                      u\"\\U0001F680-\\U0001F6FF\"  \n",
    "                      u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "                      u\"\\U00002500-\\U00002BEF\"  \n",
    "                      u\"\\U00002702-\\U000027B0\"\n",
    "                      u\"\\U00002702-\\U000027B0\"\n",
    "                      u\"\\U000024C2-\\U0001F251\"\n",
    "                      u\"\\U0001f926-\\U0001f937\"\n",
    "                      u\"\\U00010000-\\U0010ffff\"\n",
    "                      u\"\\u2640-\\u2642\"\n",
    "                      u\"\\u2600-\\u2B55\"\n",
    "                      u\"\\u200d\"\n",
    "                      u\"\\u23cf\"\n",
    "                      u\"\\u23e9\"\n",
    "                      u\"\\u231a\"\n",
    "                      u\"\\ufe0f\"  # dingbats\n",
    "                      u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "\n",
    "    tweet = re.sub(emojis, '', tweet)               # remove os emojis\n",
    "    tweet = tweet.replace('\"', \"\")                  # remove as aspas\n",
    "    tweet = re.sub(\"[-*!,$><:.+?=]\", '', tweet)     # remove demais caracteres especiais\n",
    "    tweet = re.sub(r'  ', ' ', tweet)               # remove espaços duplos\n",
    "    return tweet.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_replies(reply):\n",
    "        tweet_text = remove_special_characters(reply)\n",
    "        tweet_text = re.sub(r'@\\S+', '', tweet_text)\n",
    "        # Quando houver retweet\n",
    "        if (tweet_text[0] == 'r'):\n",
    "            tweet_text = tweet_text[4:]\n",
    "        return tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meu nivel de amizade com isis e ela ter meu in...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>o cara adultera dados que foram desmascarados...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>o cara so e simplesmente o maior vencedor da...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eu to chorando vei vsf e eu nem staneio izone ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eleitor do bolsonaro e tao ignorante q nao per...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  meu nivel de amizade com isis e ela ter meu in...    1.0\n",
       "1   o cara adultera dados que foram desmascarados...    1.0\n",
       "2    o cara so e simplesmente o maior vencedor da...    1.0\n",
       "3  eu to chorando vei vsf e eu nem staneio izone ...    1.0\n",
       "4  eleitor do bolsonaro e tao ignorante q nao per...    1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(write_replies)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bruxaria 1\n",
    "import hunspell\n",
    "\n",
    "def tokenize(sentence):\n",
    "    tokens_regex = re.compile(r\"([., :;\\n()\\\"!?\\/&%+])\", flags=re.IGNORECASE)\n",
    "    tokens = re.split(tokens_regex, sentence)\n",
    "    postprocess = []\n",
    "    postprocess_regex = re.compile(r\"\\b(\\w+)-(me|te|se|nos|vos|o|os|a|as|lo|los|la|las|lhe|lhes|lha|lhas|lho|lhos|no|na|nas|mo|ma|mos|mas|to|ta|tos|tas)\\b\", flags=re.IGNORECASE)\n",
    "    for token in tokens:\n",
    "        for token2 in re.split(postprocess_regex, token):\n",
    "            if token2.strip():\n",
    "                postprocess.append(token2)\n",
    "\n",
    "    return postprocess\n",
    "\n",
    "h = hunspell.Hunspell(\"pt_PT\", hunspell_data_dir=\"./hunspell-pt_PT-20210608/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmetization(x):\n",
    "    tokens = tokenize(x)\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        lemma = h.stem(token)\n",
    "        if len(lemma) == 1:\n",
    "            lemmas.append(lemma[0])\n",
    "        else:\n",
    "            lemmas.append(token)\n",
    "\n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3712</th>\n",
       "      <td>todo os dia cair pelo menos 10 contas de amigo...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8944</th>\n",
       "      <td>quando voce este feliz voce apreciar a musicam...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11679</th>\n",
       "      <td>achar ridiculo pessoa sem maturidade senso e p...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15404</th>\n",
       "      <td>pqp viu</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17980</th>\n",
       "      <td>eu to muito ansioso pqp terca chega logo caraio</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11266</th>\n",
       "      <td>ontem a noite encontrar alguem do tempo do esc...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13128</th>\n",
       "      <td>chupar esse picole r800 chupar meu pau diamant...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15549</th>\n",
       "      <td>a garotinha tem 8ano um puta gosto musical e u...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9095</th>\n",
       "      <td>tao duro rezar pra chover vai toma no cu pra l...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17591</th>\n",
       "      <td>ate um dia atras tinha falar que o jk so sair ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>no a fazenda 11 e um otima participante ja mos...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10694</th>\n",
       "      <td>continuar achar o maximo o look de caralho e v...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17439</th>\n",
       "      <td>nao estava fazer muito questao mas cacete em m...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4810</th>\n",
       "      <td>o ruim que no tem nenhum outro para representa...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3808</th>\n",
       "      <td>lembrar do aula de filosofia professor explica...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7289</th>\n",
       "      <td>duvidar eu ficar bater cabeca so pq o meu boy ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>e vc um salafrario que so quer o brinquedo de ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8929</th>\n",
       "      <td>agora que surf entrar no olimpiadas nenhum pai...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6231</th>\n",
       "      <td>vsf como todo mundo simples todo mundo consegu...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>as coisa nunca dao certo pra mim mesmo eu fica...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  toxic\n",
       "3712   todo os dia cair pelo menos 10 contas de amigo...    0.0\n",
       "8944   quando voce este feliz voce apreciar a musicam...    0.0\n",
       "11679  achar ridiculo pessoa sem maturidade senso e p...    1.0\n",
       "15404                                            pqp viu    0.0\n",
       "17980    eu to muito ansioso pqp terca chega logo caraio    0.0\n",
       "11266  ontem a noite encontrar alguem do tempo do esc...    0.0\n",
       "13128  chupar esse picole r800 chupar meu pau diamant...    1.0\n",
       "15549  a garotinha tem 8ano um puta gosto musical e u...    0.0\n",
       "9095   tao duro rezar pra chover vai toma no cu pra l...    1.0\n",
       "17591  ate um dia atras tinha falar que o jk so sair ...    0.0\n",
       "931    no a fazenda 11 e um otima participante ja mos...    0.0\n",
       "10694  continuar achar o maximo o look de caralho e v...    1.0\n",
       "17439  nao estava fazer muito questao mas cacete em m...    0.0\n",
       "4810   o ruim que no tem nenhum outro para representa...    1.0\n",
       "3808   lembrar do aula de filosofia professor explica...    0.0\n",
       "7289   duvidar eu ficar bater cabeca so pq o meu boy ...    0.0\n",
       "657    e vc um salafrario que so quer o brinquedo de ...    1.0\n",
       "8929   agora que surf entrar no olimpiadas nenhum pai...    0.0\n",
       "6231   vsf como todo mundo simples todo mundo consegu...    0.0\n",
       "119    as coisa nunca dao certo pra mim mesmo eu fica...    0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(lambda x: lemmetization(x))\n",
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/augusto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remoção das stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('portuguese'))\n",
    "\n",
    "text_treated = []\n",
    "\n",
    "for reply in df.text:\n",
    "    word_list = reply.split()\n",
    "\n",
    "    for word in word_list:\n",
    "        if(word in stopwords):\n",
    "            word_list.remove(word)\n",
    "    word_list = \" \".join(word_list)\n",
    "    text_treated.append(word_list)\n",
    "\n",
    "i = 0\n",
    "for row in df.iterrows():\n",
    "    df.iloc[i, 0] = text_treated[i]\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'ao',\n",
       " 'aos',\n",
       " 'aquela',\n",
       " 'aquelas',\n",
       " 'aquele',\n",
       " 'aqueles',\n",
       " 'aquilo',\n",
       " 'as',\n",
       " 'até',\n",
       " 'com',\n",
       " 'como',\n",
       " 'da',\n",
       " 'das',\n",
       " 'de',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'dele',\n",
       " 'deles',\n",
       " 'depois',\n",
       " 'do',\n",
       " 'dos',\n",
       " 'e',\n",
       " 'ela',\n",
       " 'elas',\n",
       " 'ele',\n",
       " 'eles',\n",
       " 'em',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'eram',\n",
       " 'essa',\n",
       " 'essas',\n",
       " 'esse',\n",
       " 'esses',\n",
       " 'esta',\n",
       " 'estamos',\n",
       " 'estas',\n",
       " 'estava',\n",
       " 'estavam',\n",
       " 'este',\n",
       " 'esteja',\n",
       " 'estejam',\n",
       " 'estejamos',\n",
       " 'estes',\n",
       " 'esteve',\n",
       " 'estive',\n",
       " 'estivemos',\n",
       " 'estiver',\n",
       " 'estivera',\n",
       " 'estiveram',\n",
       " 'estiverem',\n",
       " 'estivermos',\n",
       " 'estivesse',\n",
       " 'estivessem',\n",
       " 'estivéramos',\n",
       " 'estivéssemos',\n",
       " 'estou',\n",
       " 'está',\n",
       " 'estávamos',\n",
       " 'estão',\n",
       " 'eu',\n",
       " 'foi',\n",
       " 'fomos',\n",
       " 'for',\n",
       " 'fora',\n",
       " 'foram',\n",
       " 'forem',\n",
       " 'formos',\n",
       " 'fosse',\n",
       " 'fossem',\n",
       " 'fui',\n",
       " 'fôramos',\n",
       " 'fôssemos',\n",
       " 'haja',\n",
       " 'hajam',\n",
       " 'hajamos',\n",
       " 'havemos',\n",
       " 'hei',\n",
       " 'houve',\n",
       " 'houvemos',\n",
       " 'houver',\n",
       " 'houvera',\n",
       " 'houveram',\n",
       " 'houverei',\n",
       " 'houverem',\n",
       " 'houveremos',\n",
       " 'houveria',\n",
       " 'houveriam',\n",
       " 'houvermos',\n",
       " 'houverá',\n",
       " 'houverão',\n",
       " 'houveríamos',\n",
       " 'houvesse',\n",
       " 'houvessem',\n",
       " 'houvéramos',\n",
       " 'houvéssemos',\n",
       " 'há',\n",
       " 'hão',\n",
       " 'isso',\n",
       " 'isto',\n",
       " 'já',\n",
       " 'lhe',\n",
       " 'lhes',\n",
       " 'mais',\n",
       " 'mas',\n",
       " 'me',\n",
       " 'mesmo',\n",
       " 'meu',\n",
       " 'meus',\n",
       " 'minha',\n",
       " 'minhas',\n",
       " 'muito',\n",
       " 'na',\n",
       " 'nas',\n",
       " 'nem',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'nossa',\n",
       " 'nossas',\n",
       " 'nosso',\n",
       " 'nossos',\n",
       " 'num',\n",
       " 'numa',\n",
       " 'não',\n",
       " 'nós',\n",
       " 'o',\n",
       " 'os',\n",
       " 'ou',\n",
       " 'para',\n",
       " 'pela',\n",
       " 'pelas',\n",
       " 'pelo',\n",
       " 'pelos',\n",
       " 'por',\n",
       " 'qual',\n",
       " 'quando',\n",
       " 'que',\n",
       " 'quem',\n",
       " 'se',\n",
       " 'seja',\n",
       " 'sejam',\n",
       " 'sejamos',\n",
       " 'sem',\n",
       " 'serei',\n",
       " 'seremos',\n",
       " 'seria',\n",
       " 'seriam',\n",
       " 'será',\n",
       " 'serão',\n",
       " 'seríamos',\n",
       " 'seu',\n",
       " 'seus',\n",
       " 'somos',\n",
       " 'sou',\n",
       " 'sua',\n",
       " 'suas',\n",
       " 'são',\n",
       " 'só',\n",
       " 'também',\n",
       " 'te',\n",
       " 'tem',\n",
       " 'temos',\n",
       " 'tenha',\n",
       " 'tenham',\n",
       " 'tenhamos',\n",
       " 'tenho',\n",
       " 'terei',\n",
       " 'teremos',\n",
       " 'teria',\n",
       " 'teriam',\n",
       " 'terá',\n",
       " 'terão',\n",
       " 'teríamos',\n",
       " 'teu',\n",
       " 'teus',\n",
       " 'teve',\n",
       " 'tinha',\n",
       " 'tinham',\n",
       " 'tive',\n",
       " 'tivemos',\n",
       " 'tiver',\n",
       " 'tivera',\n",
       " 'tiveram',\n",
       " 'tiverem',\n",
       " 'tivermos',\n",
       " 'tivesse',\n",
       " 'tivessem',\n",
       " 'tivéramos',\n",
       " 'tivéssemos',\n",
       " 'tu',\n",
       " 'tua',\n",
       " 'tuas',\n",
       " 'tém',\n",
       " 'tínhamos',\n",
       " 'um',\n",
       " 'uma',\n",
       " 'você',\n",
       " 'vocês',\n",
       " 'vos',\n",
       " 'à',\n",
       " 'às',\n",
       " 'é',\n",
       " 'éramos'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.440714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.496485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              toxic\n",
       "count  21000.000000\n",
       "mean       0.440714\n",
       "std        0.496485\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        1.000000\n",
       "max        1.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove as colunas caso ocorra algum NaN\n",
    "df.dropna(inplace=True)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define qual coluna cada variável receberá\n",
    "x = df.text\n",
    "y = df.toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21000,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O CountVectorizer conta quantas vezes cada palavra aparece em cada text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=3000)\n",
    "x_vectorized = vectorizer.fit_transform(x.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../visualization/models/Vectorizer_XGBoost.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_fitted = vectorizer.fit(x.values)\n",
    "dump(vectorizer_fitted, '../visualization/models/Vectorizer_XGBoost.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:48:49] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:48:49] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:48:50] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:48:50] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:48:51] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:48:51] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:48:51] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:48:52] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:48:52] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:48:52] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier(use_label_encoder=False)\n",
    "targets = y_train.values\n",
    "scores = cross_validate(model, x_train, y_train, cv=10, \n",
    "                        scoring=('f1', 'accuracy','precision'), return_train_score=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.6991580877977952\n",
      "acc = 0.7404761904761905\n",
      "precision = 0.7149470563051668\n"
     ]
    }
   ],
   "source": [
    "print(f\"F1 = {scores['test_f1'].mean()}\")\n",
    "print(f\"acc = {scores['test_accuracy'].mean()}\")\n",
    "print(f\"precision = {scores['test_precision'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:50:00] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', use_label_encoder=False,\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_vectorized,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../visualization/models/XGBoost.joblib']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n",
    "dump(model, '../visualization/models/XGBoost.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern = 'rbf'\n",
    "model = SVC(kernel=kern)\n",
    "model.fit(x_vectorized, y)\n",
    "dump(model, f'../visualization/models/SVM_kernel_{kern}.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern = 'linear'\n",
    "model = SVC(kernel=kern)\n",
    "model.fit(x_vectorized, y)\n",
    "dump(model, f'../visualization/models/SVM_kernel_{kern}.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern = 'sigmoid'\n",
    "model = SVC(kernel=kern)\n",
    "model.fit(x_vectorized, y)\n",
    "dump(model, f'../visualization/models/SVM_kernel_{kern}.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vh83lstnebwG"
   },
   "outputs": [],
   "source": [
    "def retira(sentence):\n",
    "    new = [palavra for palavra in sentence.split() if palavra not in stopwords]\n",
    "    return \" \".join(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aw9hoXoJhq8y"
   },
   "outputs": [],
   "source": [
    "df.text = df.text.apply(retira)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QWz6nm-Xh1ur"
   },
   "outputs": [],
   "source": [
    "x = df.text\n",
    "y = df.toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ixzd_oTNiiRq"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "x_vectorized = vectorizer.fit_transform(x.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wExzHSiojaVi",
    "outputId": "5a3d0194-788c-4002-a87a-d827fd3aa77f"
   },
   "outputs": [],
   "source": [
    "x_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "tg64XnUwjbyD"
   },
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x_vectorized, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoX0_L9AjfG5",
    "outputId": "00ab708c-50fd-4856-b1e5-22319106bd3b"
   },
   "outputs": [],
   "source": [
    "model = SVC(kernel='rbf')\n",
    "targets = y_train.values\n",
    "scores = cross_validate(model, x_train, y_train, cv=10, \n",
    "                        scoring=('f1', 'accuracy','precision'), return_train_score=True)\n",
    "print(f\"F1 = {scores['test_f1'].mean()}\")\n",
    "print(f\"acc = {scores['test_accuracy'].mean()}\")\n",
    "print(f\"precision = {scores['test_precision'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mLtkJdvzkcpL",
    "outputId": "9b48d1ea-160b-4229-d90f-7b4bcf279a3e"
   },
   "outputs": [],
   "source": [
    "model = SVC(kernel='linear')\n",
    "targets = y_train.values\n",
    "scores = cross_validate(model, x_train, y_train, cv=10, \n",
    "                        scoring=('f1', 'accuracy','precision'), return_train_score=True)\n",
    "print(f\"F1 = {scores['test_f1'].mean()}\")\n",
    "print(f\"acc = {scores['test_accuracy'].mean()}\")\n",
    "print(f\"precision = {scores['test_precision'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8YsKv4NcnX90",
    "outputId": "a8eedfac-baa2-4d6d-9c6e-da608d446db2"
   },
   "outputs": [],
   "source": [
    "model = SVC(kernel='sigmoid')\n",
    "targets = y_train.values\n",
    "scores = cross_validate(model, x_train, y_train, cv=10, \n",
    "                        scoring=('f1', 'accuracy','precision'), return_train_score=True)\n",
    "print(f\"F1 = {scores['test_f1'].mean()}\")\n",
    "print(f\"acc = {scores['test_accuracy'].mean()}\")\n",
    "print(f\"precision = {scores['test_precision'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iyK3SRHyUWez",
    "outputId": "24e3c21e-ffe9-4df1-a3e6-8d8ffb2d8718"
   },
   "outputs": [],
   "source": [
    "model = SVC(kernel='poly', degree=2)\n",
    "targets = y_train.values\n",
    "scores = cross_validate(model, x_train, y_train, cv=10, \n",
    "                        scoring=('f1', 'accuracy','precision'), return_train_score=True)\n",
    "print(f\"F1 = {scores['test_f1'].mean()}\")\n",
    "print(f\"acc = {scores['test_accuracy'].mean()}\")\n",
    "print(f\"precision = {scores['test_precision'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xWiIJT6SUnzH",
    "outputId": "4e46a1ef-af61-40c3-914f-4cb544cfa733"
   },
   "outputs": [],
   "source": [
    "model = SVC(kernel='poly', degree=3)\n",
    "targets = y_train.values\n",
    "scores = cross_validate(model, x_train, y_train, cv=10, \n",
    "                        scoring=('f1', 'accuracy','precision'), return_train_score=True)\n",
    "print(f\"F1 = {scores['test_f1'].mean()}\")\n",
    "print(f\"acc = {scores['test_accuracy'].mean()}\")\n",
    "print(f\"precision = {scores['test_precision'].mean()}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "svm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

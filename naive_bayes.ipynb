{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uDP2RKyc88H_",
    "outputId": "e82426c4-de2d-4ae3-b028-6cd8468f3d7a"
   },
   "outputs": [],
   "source": [
    "!wget https://natura.di.uminho.pt/download/TGZ/Dictionaries/hunspell/LATEST/hunspell-pt_PT-latest.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAHEEquY9RxV"
   },
   "outputs": [],
   "source": [
    "!tar xzf hunspell-pt_PT-latest.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "id": "TROK1p9za0E3",
    "outputId": "d927b084-52bb-4655-e647-5205b46def10"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meu nivel de amizade com isis é ela ter meu in...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>o cara adultera dados que foram desmascarados...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>o cara só é simplesmente o maior vencedor d...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eu to chorando vei vsf e eu nem staneio izone ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eleitor do bolsonaro é tão ignorante q não per...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vai responder as outras 75 conversas e para de...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tem um do jack com a msm música e agr não sei ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mais é ruim pra pedir desafio esse técnico do ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>eu fico vendo isso e penso desvantagens do kp...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>frio do caralho parece até q to dentro do teu ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  meu nivel de amizade com isis é ela ter meu in...    1.0\n",
       "1   o cara adultera dados que foram desmascarados...    1.0\n",
       "2     o cara só é simplesmente o maior vencedor d...    1.0\n",
       "3  eu to chorando vei vsf e eu nem staneio izone ...    1.0\n",
       "4  eleitor do bolsonaro é tão ignorante q não per...    1.0\n",
       "5  vai responder as outras 75 conversas e para de...    1.0\n",
       "6  tem um do jack com a msm música e agr não sei ...    0.0\n",
       "7  mais é ruim pra pedir desafio esse técnico do ...    1.0\n",
       "8   eu fico vendo isso e penso desvantagens do kp...    1.0\n",
       "9  frio do caralho parece até q to dentro do teu ...    1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leitura do csv com as stop words removidas\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"comentarios_tratados.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "id": "bZ-hHGoPx-qs",
    "outputId": "d7247246-f879-4ddb-a828-c68f5476f725"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker(language='pt')\n",
    "df['text'].apply(lambda x: \" \".join([spell.correction(i) for i in x.split()]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "GxllTMXHtfOl",
    "outputId": "ff0d8d4a-b0c3-4301-f9a3-9c55fa086865"
   },
   "outputs": [],
   "source": [
    "df[\"toxic\"] = df[\"homophobia\"] + df[\"obscene\"] + df[\"insult\"] + df[\"racism\"] + df[\"misogyny\"] + df[\"xenophobia\"]\n",
    "df.drop(['obscene','insult','racism','misogyny','xenophobia','homophobia'],axis=1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "kkeXEKiBvyCW",
    "outputId": "0bca07c0-9117-4618-e8fe-c8e71e26918c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meu nivel de amizade com isis e ela ter meu in...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>o cara adultera dados que foram desmascarados...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>o cara so e simplesmente o maior vencedor d...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eu to chorando vei vsf e eu nem staneio izone ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eleitor do bolsonaro e tao ignorante q nao per...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  meu nivel de amizade com isis e ela ter meu in...    1.0\n",
       "1   o cara adultera dados que foram desmascarados...    1.0\n",
       "2     o cara so e simplesmente o maior vencedor d...    1.0\n",
       "3  eu to chorando vei vsf e eu nem staneio izone ...    1.0\n",
       "4  eleitor do bolsonaro e tao ignorante q nao per...    1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remoção de acentos\n",
    "import unicodedata\n",
    "df['text'] = df['text'].apply(lambda x: unicodedata.normalize('NFKD', x).encode('ASCII','ignore').decode('ASCII'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "EfJhRa3jtfZ_",
    "outputId": "e55ab12a-1584-467f-869e-d40e1b3e968b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meu nivel de amizade com isis e ela ter meu in...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>o cara adultera dados que foram desmascarados...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>o cara so e simplesmente o maior vencedor d...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eu to chorando vei vsf e eu nem staneio izone ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eleitor do bolsonaro e tao ignorante q nao per...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  meu nivel de amizade com isis e ela ter meu in...    1.0\n",
       "1   o cara adultera dados que foram desmascarados...    1.0\n",
       "2     o cara so e simplesmente o maior vencedor d...    1.0\n",
       "3  eu to chorando vei vsf e eu nem staneio izone ...    1.0\n",
       "4  eleitor do bolsonaro e tao ignorante q nao per...    1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cria uma classificação binária para os dados tóxicos e os não tóxicos\n",
    "df.loc[df['toxic'] == 0, 'toxic'] = 0\n",
    "df.loc[df['toxic'] > 0, 'toxic'] = 1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lqydv90o798p",
    "outputId": "1fe47084-0c3e-4ac8-8f08-fb5e798aba9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: cyhunspell in /home/augusto/.local/lib/python3.8/site-packages (2.0.2)\n",
      "Requirement already satisfied: cacheman>=2.0.6 in /home/augusto/.local/lib/python3.8/site-packages (from cyhunspell) (2.1.0)\n",
      "Requirement already satisfied: future>=0.16.0 in /usr/lib/python3/dist-packages (from cacheman>=2.0.6->cyhunspell) (0.18.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from cacheman>=2.0.6->cyhunspell) (1.15.0)\n",
      "Requirement already satisfied: psutil>=2.1.0 in /home/augusto/.local/lib/python3.8/site-packages (from cacheman>=2.0.6->cyhunspell) (5.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install cyhunspell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "p5z4IciWtwEY"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_special_characters(tweet):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)       # remove os links do tweet\n",
    "    tweet = re.sub(r't.co\\S+', '', tweet)       # remove os links \n",
    "    tweet = re.sub(r'#\\S+', '', tweet)          # remove hashtags\n",
    "    tweet = re.sub(r\"[.]\\s+\", '', tweet)        # remove reticências\n",
    "    emojis = re.compile(\"[\"                     # remove os emojis\n",
    "                      u\"\\U0001F600-\\U0001F64F\"  \n",
    "                      u\"\\U0001F300-\\U0001F5FF\"  \n",
    "                      u\"\\U0001F680-\\U0001F6FF\"  \n",
    "                      u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "                      u\"\\U00002500-\\U00002BEF\"  \n",
    "                      u\"\\U00002702-\\U000027B0\"\n",
    "                      u\"\\U00002702-\\U000027B0\"\n",
    "                      u\"\\U000024C2-\\U0001F251\"\n",
    "                      u\"\\U0001f926-\\U0001f937\"\n",
    "                      u\"\\U00010000-\\U0010ffff\"\n",
    "                      u\"\\u2640-\\u2642\"\n",
    "                      u\"\\u2600-\\u2B55\"\n",
    "                      u\"\\u200d\"\n",
    "                      u\"\\u23cf\"\n",
    "                      u\"\\u23e9\"\n",
    "                      u\"\\u231a\"\n",
    "                      u\"\\ufe0f\"  # dingbats\n",
    "                      u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "\n",
    "    tweet = re.sub(emojis, '', tweet)               # remove os emojis\n",
    "    tweet = tweet.replace('\"', \"\")                  # remove as aspas\n",
    "    tweet = re.sub(\"[-*!,$><:.+?=]\", '', tweet)     # remove demais caracteres especiais\n",
    "    tweet = re.sub(r'  ', ' ', tweet)               # remove espaços duplos\n",
    "    return tweet.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qsWCSeTntwIt"
   },
   "outputs": [],
   "source": [
    "def write_replies(reply):\n",
    "        tweet_text = remove_special_characters(reply)\n",
    "        tweet_text = re.sub(r'@\\S+', '', tweet_text)\n",
    "        # Quando houver retweet\n",
    "        if (tweet_text[0] == 'r'):\n",
    "            tweet_text = tweet_text[4:]\n",
    "        return tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "JfjlAQeltwMr",
    "outputId": "3823024e-0e2c-44fd-a9ab-cf7756a46170"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meu nivel de amizade com isis e ela ter meu in...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>o cara adultera dados que foram desmascarados...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>o cara so e simplesmente o maior vencedor da...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eu to chorando vei vsf e eu nem staneio izone ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eleitor do bolsonaro e tao ignorante q nao per...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  meu nivel de amizade com isis e ela ter meu in...    1.0\n",
       "1   o cara adultera dados que foram desmascarados...    1.0\n",
       "2    o cara so e simplesmente o maior vencedor da...    1.0\n",
       "3  eu to chorando vei vsf e eu nem staneio izone ...    1.0\n",
       "4  eleitor do bolsonaro e tao ignorante q nao per...    1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(write_replies)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fJfxHuqA6eUC"
   },
   "outputs": [],
   "source": [
    "# Bruxaria 1\n",
    "import hunspell\n",
    "\n",
    "def tokenize(sentence):\n",
    "    tokens_regex = re.compile(r\"([., :;\\n()\\\"!?\\/&%+])\", flags=re.IGNORECASE)\n",
    "    tokens = re.split(tokens_regex, sentence)\n",
    "    postprocess = []\n",
    "    postprocess_regex = re.compile(r\"\\b(\\w+)-(me|te|se|nos|vos|o|os|a|as|lo|los|la|las|lhe|lhes|lha|lhas|lho|lhos|no|na|nas|mo|ma|mos|mas|to|ta|tos|tas)\\b\", flags=re.IGNORECASE)\n",
    "    for token in tokens:\n",
    "        for token2 in re.split(postprocess_regex, token):\n",
    "            if token2.strip():\n",
    "                postprocess.append(token2)\n",
    "\n",
    "    return postprocess\n",
    "\n",
    "h = hunspell.Hunspell(\"pt_PT\", hunspell_data_dir=\"./hunspell-pt_PT-20210608/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ty8zbbkm7oEt"
   },
   "outputs": [],
   "source": [
    "def lemmetization(x):\n",
    "    tokens = tokenize(x)\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        lemma = h.stem(token)\n",
    "        if len(lemma) == 1:\n",
    "            lemmas.append(lemma[0])\n",
    "        else:\n",
    "            lemmas.append(token)\n",
    "\n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 648
    },
    "id": "SmEwLPvN-O2T",
    "outputId": "6f62a937-d4ae-4844-ef84-da66ac30c8df"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13671</th>\n",
       "      <td>mais feio que eu hj so eu ontem</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4314</th>\n",
       "      <td>achar que eu sou a unica pessoa de campo que n...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4240</th>\n",
       "      <td>quem e esse passa visao que vou mandar passar ele</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16880</th>\n",
       "      <td>kkkkkkkkkk que povo fudido</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6977</th>\n",
       "      <td>yoonminau [nsfw] onde jimin mandar um nude sem...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6182</th>\n",
       "      <td>os melhores viloes ever passar no sua tl</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11839</th>\n",
       "      <td>o que que tem terca cara de pica</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>to eu aqui no oficina e os cara do loja do lad...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15751</th>\n",
       "      <td>eu abel vai se fuder o meu flamengo nao precis...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16047</th>\n",
       "      <td>putinha aguentar 3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5081</th>\n",
       "      <td>mil favor muito coisa nu</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17721</th>\n",
       "      <td>eu ler ele inteira e nao e total ruim e nem e ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2727</th>\n",
       "      <td>parabens rayssa a unica coisa q vc sabe fazer ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>precisar ir pra cima do stf segunda feira vao ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8100</th>\n",
       "      <td>me abandonar igual o outro fds fdp</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17445</th>\n",
       "      <td>cara vc to ferrado c a horda bolsonaristastodo...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6314</th>\n",
       "      <td>e acabar</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19764</th>\n",
       "      <td>os fato pqp</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>vei n existir um defeito nesse porra</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17467</th>\n",
       "      <td>tudo ser crianca muito melhor pois eu nem sair...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  toxic\n",
       "13671                    mais feio que eu hj so eu ontem    1.0\n",
       "4314   achar que eu sou a unica pessoa de campo que n...    0.0\n",
       "4240   quem e esse passa visao que vou mandar passar ele    0.0\n",
       "16880                         kkkkkkkkkk que povo fudido    1.0\n",
       "6977   yoonminau [nsfw] onde jimin mandar um nude sem...    0.0\n",
       "6182            os melhores viloes ever passar no sua tl    0.0\n",
       "11839                   o que que tem terca cara de pica    1.0\n",
       "670    to eu aqui no oficina e os cara do loja do lad...    0.0\n",
       "15751  eu abel vai se fuder o meu flamengo nao precis...    1.0\n",
       "16047                                 putinha aguentar 3    1.0\n",
       "5081                            mil favor muito coisa nu    0.0\n",
       "17721  eu ler ele inteira e nao e total ruim e nem e ...    0.0\n",
       "2727   parabens rayssa a unica coisa q vc sabe fazer ...    1.0\n",
       "2084   precisar ir pra cima do stf segunda feira vao ...    0.0\n",
       "8100                  me abandonar igual o outro fds fdp    1.0\n",
       "17445  cara vc to ferrado c a horda bolsonaristastodo...    1.0\n",
       "6314                                            e acabar    0.0\n",
       "19764                                        os fato pqp    0.0\n",
       "1602                vei n existir um defeito nesse porra    1.0\n",
       "17467  tudo ser crianca muito melhor pois eu nem sair...    0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(lambda x: lemmetization(x))\n",
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u93myWwKbyuo",
    "outputId": "12bf6178-322b-4a7e-9398-c8aba808e6b0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/augusto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "APxO-stHElh6"
   },
   "outputs": [],
   "source": [
    "# Remoção das stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('portuguese'))\n",
    "\n",
    "text_treated = []\n",
    "\n",
    "for reply in df.text:\n",
    "    word_list = reply.split()\n",
    "\n",
    "    for word in word_list:\n",
    "        if(word in stopwords):\n",
    "            word_list.remove(word)\n",
    "    word_list = \" \".join(word_list)\n",
    "    text_treated.append(word_list)\n",
    "\n",
    "i = 0\n",
    "for row in df.iterrows():\n",
    "    df.iloc[i, 0] = text_treated[i]\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 648
    },
    "id": "Zokyhx5mE2Ir",
    "outputId": "08460423-b8c8-4c67-9929-f8d9a4101cce"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10171</th>\n",
       "      <td>so feio dms</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1866</th>\n",
       "      <td>vzs me botar lugar meu vizinho tipo ficar puto...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14416</th>\n",
       "      <td>vai fuder amigo</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>evidente canalha petista culpar pleonasmo voce...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10023</th>\n",
       "      <td>adorar noit perceber q n mt amigo proximo d fa...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7290</th>\n",
       "      <td>ces comecaram mamar cara agora virar criatura ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17475</th>\n",
       "      <td>camarote comunista ralar no globo lixo</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12899</th>\n",
       "      <td>julieta que contar sobre barbar aaaa pqp</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18370</th>\n",
       "      <td>porra sei</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19485</th>\n",
       "      <td>povo nao precisa stfo povo nao escolher ningue...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12040</th>\n",
       "      <td>queria pau todo melar leite minha boca</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19972</th>\n",
       "      <td>vai tomar cu içar te odeio fikho putaaaaaaaaaaa</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5365</th>\n",
       "      <td>kkkkk desculpa no conseguir compartilhar insta...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>oxe moral q epic vai fazer jogar skin randomica</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14232</th>\n",
       "      <td>socios setor leste superior gracas deus procur...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16631</th>\n",
       "      <td>todo cachorro fdp</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>sentir dia 10 chp ja to vendo vou chorar horro...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19943</th>\n",
       "      <td>boa tarde so pra to fome cu</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>mario bittencourt time quarto final sulamerica...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9123</th>\n",
       "      <td>puta tem coisa me broxa 100 % chamar alguem pr...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  toxic\n",
       "10171                                        so feio dms    0.0\n",
       "1866   vzs me botar lugar meu vizinho tipo ficar puto...    1.0\n",
       "14416                                    vai fuder amigo    1.0\n",
       "2157   evidente canalha petista culpar pleonasmo voce...    1.0\n",
       "10023  adorar noit perceber q n mt amigo proximo d fa...    1.0\n",
       "7290   ces comecaram mamar cara agora virar criatura ...    1.0\n",
       "17475             camarote comunista ralar no globo lixo    1.0\n",
       "12899           julieta que contar sobre barbar aaaa pqp    0.0\n",
       "18370                                          porra sei    0.0\n",
       "19485  povo nao precisa stfo povo nao escolher ningue...    0.0\n",
       "12040             queria pau todo melar leite minha boca    1.0\n",
       "19972    vai tomar cu içar te odeio fikho putaaaaaaaaaaa    1.0\n",
       "5365   kkkkk desculpa no conseguir compartilhar insta...    0.0\n",
       "1442     oxe moral q epic vai fazer jogar skin randomica    0.0\n",
       "14232  socios setor leste superior gracas deus procur...    1.0\n",
       "16631                                  todo cachorro fdp    0.0\n",
       "836    sentir dia 10 chp ja to vendo vou chorar horro...    0.0\n",
       "19943                        boa tarde so pra to fome cu    1.0\n",
       "488    mario bittencourt time quarto final sulamerica...    0.0\n",
       "9123   puta tem coisa me broxa 100 % chamar alguem pr...    1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "id": "4z1CstFNlwb-",
    "outputId": "030d8270-42e4-4d1b-c742-b61279721d5c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.440714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.496485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              toxic\n",
       "count  21000.000000\n",
       "mean       0.440714\n",
       "std        0.496485\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        1.000000\n",
       "max        1.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove as colunas caso ocorra algum NaN\n",
    "df.dropna(inplace=True)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "RHTsBMvcbRMa"
   },
   "outputs": [],
   "source": [
    "# Define qual coluna cada variável receberá\n",
    "x = df.text\n",
    "y = df.toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21000,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../visualization/models/NaiveBayes_MultinomialNB.joblib']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para fins de colocar no streamlit\n",
    "from joblib import dump\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=3000)\n",
    "x_vectorized = vectorizer.fit_transform(x.values)\n",
    "vectorizer_fitted = vectorizer.fit(x.values)\n",
    "dump(vectorizer_fitted, '../visualization/models/Vectorizer_NaiveBayes_MultinomialNB.joblib')\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier_fitted = classifier.fit(x_vectorized, y)\n",
    "dump(classifier_fitted, '../visualization/models/NaiveBayes_MultinomialNB.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quais serão os dados de treino e de teste, já com o x vetorizado\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2519     no final entender porra todo me acertar a gale...\n",
       "1855                          vitao perfeito 0 defeito pqp\n",
       "11805    carinha nervoso ( mongoloide q nao sabe quer i...\n",
       "7440     achar este pesquisa nao fidedigno dever ser me...\n",
       "20349    luhan vc vai msm forcar ficar colocar foto de ...\n",
       "                               ...                        \n",
       "4910     cs fala cara desperdiou cara depresso perder p...\n",
       "19360    premiacoes coreano so tornar irrelevante parti...\n",
       "7614     hahahahahahahahahahahahahahahah conseguir ouvi...\n",
       "10403    seu nocao caralho voce acha o problema seu nao...\n",
       "2836                               ridicula kkkkkkkkkkkkkk\n",
       "Name: text, Length: 16800, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "IUH_7AUMbRbc"
   },
   "outputs": [],
   "source": [
    "# O CountVectorizer conta quantas vezes cada palavra aparece em cada e-mail\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=3000)\n",
    "x_vectorized = vectorizer.fit_transform(x_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_fitted = vectorizer.fit(x_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "nZHe7KsgbRnl"
   },
   "outputs": [],
   "source": [
    "# Instanciando o classificador Naive-Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "targets = y_train.values\n",
    "\n",
    "# Utilização da validação cruzada\n",
    "from sklearn.model_selection import cross_validate\n",
    "scores = cross_validate(classifier, x_vectorized, y_train, cv=10, scoring=('f1', 'accuracy','precision'), return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_fitted = classifier.fit(x_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_fitted.predict(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "na2YnW7Gbkyk",
    "outputId": "734f65a7-79ee-4683-a45c-81cbecf62b2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6790890571058457"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['test_f1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47vMy-6jbk5n",
    "outputId": "2cb11a19-c94e-4546-8447-c871c4fe0476"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.717202380952381"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['test_accuracy'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ykviAsg4blA1",
    "outputId": "01cc03f9-ee03-4ac9-a125-862791015c6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6755129316319821"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['test_precision'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../visualization/models/NaiveBayes_MultinomialNB.joblib']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(classifier_fitted, '../visualization/models/NaiveBayes_MultinomialNB.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../visualization/models/Vectorizer_NaiveBayes_MultinomialNB.joblib']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(vectorizer_fitted, '../visualization/models/Vectorizer_NaiveBayes_MultinomialNB.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_fitted = vectorizer.fit(x.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x3000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_fitted.transform(x_test.sample(1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['lugar onde me sinto desconfortavel bem vinda no propria casa pode porra desse mano'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "naive_bayes_without_stopwords.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "4e1d9a8909477db77738c33245c29c7265277ef753467dede8cf3f814cde494e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
